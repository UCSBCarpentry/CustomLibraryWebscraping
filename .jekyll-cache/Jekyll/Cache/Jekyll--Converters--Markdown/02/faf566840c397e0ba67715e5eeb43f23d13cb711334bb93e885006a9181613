I";<p>Going further:</p>
<ul>
  <li>Pipelines
  Can be used to check validity of returned data
  http://doc.scrapy.org/en/latest/topics/item-pipeline.html</li>
</ul>

<p>CLOUD-BASED SCRAPING</p>
<ul>
  <li>ScrapingHub, Scrapy Cloud</li>
  <li>Morph.io</li>
</ul>

<p>Example deploy on ScrapingHub</p>

<ul>
  <li>
    <p>Login/Register on https://app.scrapinghub.com/
  Use Google account</p>
  </li>
  <li>
    <p>If not done already, run
  $ pip install shub</p>
  </li>
  <li>
    <p>$ shub login
  Provide your API as found on https://app.scrapinghub.com/account/apikey
  This writes the API key into a local file: less ~/.scrapinghub.yml
  Not specific to that repository 
  shub logout to remove it</p>
  </li>
  <li>Create a new project, note its ID</li>
  <li>
    <p>cd to the root of a scrapy project directory</p>
  </li>
  <li>
    <p>$ shub deploy
  Provide the ID of the recently created project</p>
  </li>
  <li>
    <p>From the web app, run spider
  Arguments can be provided</p>
  </li>
  <li>
    <p>Items are found under the Items tab
  Free project: one concurrent spider, max 2.5GB of data storage, data retention 7 days
  Items can be downloaded individually or in batch (big green Items button on top)
  Can also be accessed through API calls
      http://doc.scrapinghub.com/api/overview.html
      e.g. curl -u 1e2490bfc15d4e6089e4b842364b5cd1: “https://storage.scrapinghub.com/items/85589/1/1?format=xml”</p>
  </li>
  <li>Jobs can be scheduled, etc.</li>
</ul>

<p>They have a GUI (work in progress) to define elements and build spiders: Portia</p>

<p>CHECK IF ENOUGH TIME FOR LEGAL ASPECTS</p>

<p>ALTERNATIVE: MORPH.IO</p>
<ul>
  <li>Port of ScraperWiki Classic</li>
  <li>Open source</li>
  <li>Project supported by the OpenAustralia Foundation</li>
  <li>Can be run locally, but also on cloud platform morph.io “Heroku for scrapers”- very convenient - works with GitHub
  requires a working install of git
  requires ruby or python + several libraries for local running</li>
  <li>For this reason, not chosen for this workshop</li>
  <li>Platform runs the scraper and also stores data
  Data can easily be shared, downloaded in structured format and accessed through dedicated API</li>
  <li>Demo if enough time</li>
</ul>

<p>CHECK IF ENOUGH TIME FOR LEGAL ASPECTS</p>

<p>SCRAPE DATA FROM PDFs</p>

<p>Use Tabula
	- Free, open source software
	- Available for Mac, Windows, Linux
	- Runs in browser (much like OpenRefine)
	https://github.com/tabulapdf/tabula</p>

<p>This example uses a library that converts PDF to XML, then does the extraction.</p>

<p>http://www.bl.uk/reshelp/atyourdesk/docsupply/help/replycodes/dirlibcodes/
https://morph.io/ostephens/british_library_directory_of_library_codes
	British Library maintains list of library codes for its Document Supply service.
	List of codes in a PDF file -&gt; UNSTRUCTURED</p>

:ET